# -*- coding: utf-8 -*-
"""YB_SVM.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Oy_SD487PRkTWgfhg1ArXe6vl5CaXKpu
"""

from google.colab import drive
drive.mount('/content/drive')

from sklearn import svm
from __future__ import absolute_import, division, print_function, unicode_literals

import _pickle as cPickle
import numpy as np
import argparse
from tempfile import TemporaryFile
#import tensorflow as tf
#from tensorflow import keras


#outfile = TemporaryFile()

print("\nLabels: Valence, Arousal, Dominance, Liking\n")

num_users=1
num_entries=40 * num_users
electrodes = [1,3,4,6,7,19,21,26]#[4,19,3,1] #[3,6,10,13,16,19,20,28]
num_electrodes= len(electrodes)

VA=np.empty([num_entries, 2]) #Valence, Arousal
emotion=[0]*num_entries
k=0
entry =0
new_data= np.empty([num_entries,num_electrodes,8064]) # [40, 8 * 8064]

for name in range(1,num_users+1):

	file='/content/drive/My Drive/data_preprocessed_python/s02.dat'

	#print(file)
	x = cPickle.load(open(file, 'rb'),encoding='latin1')


	for i in range(0,40):
		VA[i][0]=(x["labels"][i][0]) #Valence
		VA[i][1]=(x["labels"][i][1]) #Arousal
		if(VA[i][0] <=5):
			#if(VA[i][1] <=3.67):
			emotion[entry]=0 #"Sad"
			#elif(VA[i][1] <=6.34):
				#emotion[entry]= 1 #"Frustrated"
			#else:
				#emotion[entry]=2 #"Fear"
		if(VA[i][0] > 5):
			#if(VA[i][1] <=3.67):
			emotion[entry]=1 #"Satisfied"
			#elif(VA[i][1] <=6.34):
				#emotion[entry]= 4 #"Pleasant"
			#else:
				#emotion[entry]=5 #"Happy"

		for j in electrodes:# [0,2,6,10,16,19,24,28]:
			#print(x["data"][i][j],"\n")
			for l in range(0,8064):
				new_data[entry][k][l]=x["data"][i][j][l]
			k+=1
		#	print("\nk,j:",k, j)
		k=0
		entry+=1
emotion_label=np.asarray(emotion)
print(emotion_label)

print(emotion_label.shape)
print(new_data.shape)
def delta(x,xlen):
	return ( np.sum(np.diff(x,n=1)) / (xlen - 1) )

def gamma(x,xlen):
	sum=0
	for i in range(0,xlen-2):
		sum += abs(x[i+2] - x[i])
	return ( sum / (xlen - 2) )

import math

'''
We divide the 8064 readings per channel, into 10
batches of approximately 807 readings each. For each batch
we extract the mean, median, maximum, minimum, standard
deviation, variance, range, skewness and kurtosis values for
the 807 readings. Hence for each of the 10 batches of a single channel we extract 9 values mentioned above, we get 90
values as our processed dataset. We further add the net mean,4/jwHALcCPDiy5sn0lpMAKNJsLH_SPnTGbWkOxS2GhFq--SYYUr9wY0X
median, maximum, minimum, standard deviation, variance,
range, skewness and kurtosis values for the entire 8064 readings along with the experiment and participant number to
our dataset, bringing it up to 101 values per channel.
'''
import scipy
from scipy import stats, optimize, interpolate

new_data_processed = np.empty([num_entries,num_electrodes,441]) # 7 * 63
index = 0
print("\nNew_data original shape:",new_data.shape)
for i in range(0,num_entries):
  for j in range(0,num_electrodes):
    index = 0
    for k in range(0,8064,128):
      if k!=7936:
        chunk = new_data[i][j][k:k+512]
      else:
        chunk = new_data[i][j][k:8065]

      chunk_len = len(chunk)

      new_data_processed[i][j][index] = np.mean(chunk)
      index += 1

      chunk_stdev = np.std(chunk) * math.sqrt(chunk_len / (chunk_len-1))
      new_data_processed[i][j][index] = chunk_stdev
      index += 1

      chunk_delta = delta(chunk,chunk_len)
      new_data_processed[i][j][index] = chunk_delta
      index += 1

      new_data_processed[i][j][index] = chunk_delta / chunk_stdev
      index += 1

      chunk_gamma = gamma(chunk,chunk_len)
      new_data_processed[i][j][index] = chunk_gamma
      index += 1

      new_data_processed[i][j][index] = chunk_gamma / chunk_stdev
      index += 1

      k=10
      #print(type(hfda.measure(chunk,k)))
      new_data_processed[i][j][index] = hfda.measure(chunk,k)
      index += 1
   # print(index)


print("\nNew data processed shape:", new_data_processed.shape)

#new_data_processed=new_data
from sklearn.utils import shuffle

train_size = math.floor(new_data_processed.shape[0] * 0.8 )
test_size = new_data_processed.shape[0] - train_size
new_data_processed,emotion_label = shuffle(new_data_processed,emotion_label)
test_y = emotion_label[train_size:train_size+test_size]
r=0
for i in range(0,len(emotion_label)):
  if emotion_label[i]==0:
    #print(emotion[i])
    r+=1
print(r)

train_x= new_data_processed[0:train_size][:][:]
train_y= emotion_label[0:train_size]
test_x= new_data_processed[train_size:train_size+test_size][:][:]
test_y= emotion_label[train_size:train_size+test_size]

print("\nTraining shape:",train_x.shape)
print("\nTesting shape:",test_x.shape)

from sklearn import svm
from sklearn.decomposition import PCA

train_x_svm = train_x.reshape((train_size,num_electrodes*441))
test_x_svm = test_x.reshape((test_size,num_electrodes*441))

print("Start reduction...")
num_components=200
print("\nNumber of compnents=",num_components)
#pca = PCA(n_components = num_components, whiten=True)
#train_x_svm = pca.fit_transform(train_x_svm)
#test_x_svm = pca.transform(test_x_svm)
print("\nFinished reduction!")

import random
from sklearn.model_selection import cross_val_score
for C in [0.001,0.01,0.1,1]:
  print("\nC=",C)
  clf = svm.SVC(C=C,degree=5,kernel='rbf',gamma=1.0, decision_function_shape='ovr')
  #‘linear’, ‘poly’, ‘rbf’, ‘sigmoid’
  '''
  scores = cross_val_score(clf, new_data_processed.reshape(new_data_processed.shape[0],num_electrodes*378), emotion_label, cv=5)
  print(scores)
  '''
  clf.fit(train_x_svm, train_y)

  op_test=clf.predict(test_x_svm)
  op_train=clf.predict(train_x_svm)
  #op1=[0]*len(op)
  print(test_y)
  print(op_test)
  count_test=0
  count_train=0

  for i in range(0,len(op_test)):
    #op1[i]=random.randint(0,5)
    if(test_y[i]==op_test[i]):
      count_test+=1
  for i in range(0,len(op_train)):
    #op1[i]=random.randint(0,5)
    if(train_y[i]==op_train[i]):
      count_train+=1

  print("\nTraining accuracy:",(count_train/len(op_train) * 1.0))


  print("\nTesting accuracy:",(count_test/len(op_test) * 1.0))