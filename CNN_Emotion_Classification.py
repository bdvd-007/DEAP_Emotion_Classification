# -*- coding: utf-8 -*-
"""replicaCNN.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1H6gjeEigevQb2NmLlx3_bC6jSTOIKa5B
"""



from __future__ import absolute_import, division, print_function, unicode_literals


import tensorflow as tf

from tensorflow.keras import datasets, layers, models

from __future__ import absolute_import, division, print_function, unicode_literals

import _pickle as cPickle
import numpy as np
import argparse
from tempfile import TemporaryFile
#import tensorflow as tf



#outfile = TemporaryFile()

print("\nLabels: Valence, Arousal, Dominance, Liking\n")

num_users = 6#32 #6
num_entries = 40 * num_users



VA=np.empty([num_entries, 2]) #Valence, Arousal
emotion=[0]*num_entries
k=0
entry =0

electrodes = [3,6,10,13,16,19,20,28]#np.arange(0,32)# [3,6,10,13,16,19,20,28]# np.arange(0,32)
num_electrodes = len(electrodes)
#num_electrodes = 32
new_data= np.empty([num_entries,num_electrodes,8064]) # [40, 8 * 8064]

for name in [7,8,10,16,19,20]:# range(1,num_users+1):#[7,8,10,16,19,20]:#range(1,num_users+1):

  if(name<10):
    file='/content/drive/My Drive/data_preprocessed_python/s0'+str(name)+'.dat'
  else:
    file='/content/drive/My Drive/data_preprocessed_python/s'+str(name)+'.dat'
  print(file)
  x = cPickle.load(open(file, 'rb'),encoding='latin1')


  for i in range(0,40):
    VA[i][0]=(x["labels"][i][0]) #Valence
    VA[i][1]=(x["labels"][i][1]) #Arousal
    if(VA[i][1] <= 4.5):
      if ( VA[i][0] <= 4.5 ) :
        emotion[entry] = 0 #"Sad"
      else:
        emotion[entry] = 1
			#elif(VA[i][1] <=6.34):
				#emotion[entry]= 1 #"Frustrated"
			#else:
				#emotion[entry]=2 #"Fear"
    if ( VA[i][1] > 4.5 ) :
      if(VA[i][0] <= 4.5):
        emotion[entry]=2#Satisfied"
      else:
        emotion[entry]=3
			#elif(VA[i][1] <=6.34):
				#emotion[entry]= 4 #"Pleasant"
			#else:
				#emotion[entry]=5 #"Happy"

    for j in electrodes: #range(0,32):#[1,3,4,6,7,19,21,26]:# range(0,num_electrodes):
      #print(x["data"][i][j],"\n")
      for l in range(0,8064):
        new_data[entry][k][l]=x["data"][i][j][l]
      k+=1
		#	print("\nk,j:",k, j)
    k=0
    entry+=1
emotion_label=np.asarray(emotion)
print(emotion_label)
print(new_data.shape)

from google.colab import drive
drive.mount('/content/drive')

import math
'''
We divide the 8064 readings per channel, into 10
batches of approximately 807 readings each. For each batch
we extract the mean, median, maximum, minimum, standard
deviation, variance, range, skewness and kurtosis values for
the 807 readings. Hence for each of the 10 batches of a single channel we extract 9 values mentioned above, we get 90
values as our processed dataset. We further add the net mean,4/jwHALcCPDiy5sn0lpMAKNJsLH_SPnTGbWkOxS2GhFq--SYYUr9wY0X
median, maximum, minimum, standard deviation, variance,
range, skewness and kurtosis values for the entire 8064 readings along with the experiment and participant number to
our dataset, bringing it up to 101 values per channel.
'''
import scipy
from scipy import stats, optimize, interpolate
new_data_processed = np.empty([num_entries * 29  ,num_electrodes,512]) # (9*63) + 9

new_emotion_labels = np.empty([num_entries * 29])

print(new_data_processed.shape)
index = 0
k=0
print("\nNew_data original shape:",new_data.shape)
for i in range(0,num_entries * 29 ):
  new_emotion_labels[i] = emotion[math.floor(i/29)]
  for j in range(0,num_electrodes):
    index = 0
    #for k in range(0,8064,128):
    if k <= 7936:
      if k!=7936:
        chunk = new_data[math.floor(i/29)][j][k:k+512]
      else:
        chunk = new_data[math.floor(i/29)][j][k:8065]


      new_data_processed[i][j][0:chunk.shape[0]] = chunk
      index += 1
      #print(index)
      k += 256
    if k == 7936:
      k = 0
print(index)

print("\nNew data processed shape:", new_data_processed.shape)

def pcc(data_):
  alpha=[]
  beta=[]
  gamma=[]
  theta=[]
  #print("\nTEST:",(data_[0].shape))
  for i in range(0,num_electrodes):
	  data = data_[i]
	  fft_vals = np.absolute(np.fft.fft(data)) #np.absolute
	  #print("\nSize:",fft_vals.size,data.size)
	  # Get frequencies for amplitudes in Hz
	  fft_freq = np.fft.fftfreq(len(data), 1.0/128)
	  #print(np.fft.rfft(x['data'][0][0])[0:10] )
	  #print("\n\nORIGINAL:",data[0:50],"\n\nFFT.RFFT:",fft_vals[0:50], "\n\nRFFT.FREQ:" ,fft_freq[0:50])
	  eeg_bands = {'Theta': (4, 7),
	             'Alpha': (8, 13),
	             'Beta': (14, 30),
	             'Gamma': (31, 50)}
	  eeg_band_fft = dict()

	  for band in eeg_bands:
		  freq_ix = np.where((fft_freq >= eeg_bands[band][0]) & (fft_freq <= eeg_bands[band][1]))[0]
		  #print(freq_ix, band)
		  #print((fft_vals[freq_ix]),band)
		  if(band == 'Alpha'):
		      alpha.append(fft_vals[freq_ix])
		  if(band == 'Beta'):
		      beta.append(fft_vals[freq_ix])
		  if(band == 'Gamma'):
		      gamma.append(fft_vals[freq_ix])
		  if(band == 'Theta'):
		      theta.append(fft_vals[freq_ix])
	  #print(i)
  #print(len(alpha), len(beta),len(gamma), len(theta))
  pcc_alpha = np.empty([num_electrodes,num_electrodes])
  pcc_beta = np.empty([num_electrodes,num_electrodes])
  pcc_gamma = np.empty([num_electrodes,num_electrodes])
  pcc_theta = np.empty([num_electrodes,num_electrodes])
  #print("\nLENGTH:",alpha[0])
  for i in range(0,num_electrodes):
    for j in range(0,num_electrodes):
      pcc_alpha[i][j] = scipy.stats.pearsonr(alpha[i],alpha[j])[0]
      pcc_beta[i][j] = scipy.stats.pearsonr(beta[i],beta[j])[0]
      pcc_gamma[i][j] = scipy.stats.pearsonr(gamma[i],gamma[j])[0]
      pcc_theta[i][j] = scipy.stats.pearsonr(theta[i],theta[j])[0]
  	#print(alpha[i])
  #print(pcc.shape)
  return pcc_alpha,pcc_beta,pcc_gamma,pcc_theta

images = np.empty([new_data_processed.shape[0],4,num_electrodes,num_electrodes])
#print(images[0][:].shape)
for i in range(0,new_data_processed.shape[0]):
  images[i][:] = np.asarray(pcc(new_data_processed[i][:][:]))

print(images.shape)

#new_data_processed= np.empty(new_data.shape)
#new_data_processed= new_data
from sklearn.utils import shuffle

test_size = math.floor(images.shape[0] * 0.1 )#116#232#256
train_size = images.shape[0] - test_size #1044#928#1024

images , new_emotion_labels = shuffle(images,new_emotion_labels)
test_y = new_emotion_labels[train_size:train_size+test_size]

r=0
for i in range(0,len(new_emotion_labels)):
  if new_emotion_labels[i]==0:
    #print(emotion[i])
    r+=1
print(r)



train_x = images[0:train_size][:][:]
train_y = new_emotion_labels[0:train_size]
test_x = images[train_size:train_size+test_size][:][:]
test_y = new_emotion_labels[train_size:train_size+test_size]

train_x = train_x.reshape((train_x.shape[0], num_electrodes,num_electrodes,4))
test_x = test_x.reshape((test_x.shape[0], num_electrodes,num_electrodes,4))

print("\nTraining shape:",train_x.shape)
print("\nTesting shape:",test_x.shape)

from tensorflow.keras.layers import Dense, Dropout, Activation

model = models.Sequential()

model.add(layers.Conv2D(8, (3, 1) , activation='relu', input_shape=( num_electrodes,num_electrodes,4),padding="same" ))

model.add(layers.Conv2D(8, (3, 1) , activation='relu',padding="same"))

#model.add(layers.AveragePooling2D((3, 1) , padding="same"))

model.add(layers.Conv2D(16, (3, 3) , activation='relu', padding="same"))

model.add(layers.Conv2D(16, (3, 3) , activation='relu' , padding="same"))

#model.add(layers.AveragePooling2D((3, 3) ))#,  padding="same"))

model.add(layers.Flatten())

model.add(layers.Dense(128, activation='relu'))

model.add(layers.Dense(4, activation='relu'))

model.summary()
'''
model = models.Sequential()

model.add(layers.Conv2D(32, (3, 1) , activation='relu', input_shape=( num_electrodes,num_electrodes,4),padding="same" ))

model.add(layers.Conv2D(32, (3, 1) , activation='relu'))#,padding="same"))

model.add(layers.AveragePooling2D((3, 1)))# , padding="same"))

model.add(layers.Conv2D(64, (3, 3) , activation='relu', padding="same"))

model.add(layers.Conv2D(64, (3, 3) , activation='relu' ))#, padding="same"))

model.add(layers.AveragePooling2D((3, 3) ))#,  padding="same"))

model.add(layers.Flatten())

model.add(layers.Dense(512, activation='relu'))

model.add(layers.Dense(4, activation='relu'))

model.summary()
'''

#model.summary()
from tensorflow import keras
"""As you can see, our (3, 3, 64) outputs were flattened into vectors of shape (576) before going through two Dense layers.

### Compile and train the model
"""

model.compile(optimizer=keras.optimizers.Adam(learning_rate=0.001),
              loss='sparse_categorical_crossentropy',
              metrics=['accuracy'])

model.fit(train_x, train_y, epochs=80, batch_size=10)



#model.fit(train_x, train_y, epochs=250, batch_size=310)

"""### Evaluate the model"""


test_loss, test_acc = model.evaluate(test_x, test_y)

print(test_acc)

test_loss, test_acc = model.evaluate(test_x, test_y)